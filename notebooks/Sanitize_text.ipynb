{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1737c066-17a7-43c0-af6d-357f6d85d956",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports and env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2612c1f2-d309-4c44-be8b-86eb7fc3c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Michael\n",
      "[nltk_data]     Minut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Michael\n",
      "[nltk_data]     Minut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Michael\n",
      "[nltk_data]     Minut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, random, os\n",
    "import json, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import seaborn as sns\n",
    "import enchant\n",
    "from pathlib import Path\n",
    "from datasets import list_datasets, load_dataset\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec86036-34d4-49ab-a92a-dec4822eed60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Michael\n",
      "[nltk_data]     Minut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2971efd1-f2d8-4424-ac84-748a55be99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ro-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ro_core_news_sm-3.4.0/ro_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from ro-core-news-sm==3.4.0) (3.4.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (3.0.11)\n",
      "Requirement already satisfied: jinja2 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (1.10.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (8.1.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (1.23.5)\n",
      "Requirement already satisfied: setuptools in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (57.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: colorama in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\coding\\repos\\dlnlpproject\\venv\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->ro-core-news-sm==3.4.0) (2.1.1)\n",
      "Installing collected packages: ro-core-news-sm\n",
      "Successfully installed ro-core-news-sm-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('ro_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'E:\\coding\\repos\\DLNLPProject\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ro_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a8efa67-f72d-4d67-887a-ab4289642d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ro_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39646c5e-c436-4520-bab5-7a82da500068",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_en = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6438c66e-0fe9-4a89-a111-f2f34086a312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_en.check(\"duh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec000f0-3c0e-4255-b9cb-5bd011c64391",
   "metadata": {},
   "source": [
    "# Sanitization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20fc5269-5b39-4f80-b9fb-76cc13f2e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Class - PreprocessExpert\n",
    "# ======================================================================================================================\n",
    "class PreprocessExpert:\n",
    "\n",
    "    RO_STOP_WORDS = None\n",
    "    EN_ACRONYMS = None\n",
    "    PORTER_STEM = None\n",
    "    WORDNET_LEMMATIZER = None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_stop_words_ro():\n",
    "        # From: https://www.kaggle.com/code/mpwolke/romanian-stop-words-w2v/data?select=romanian.txt\n",
    "        with open(r'./data_files/romanian_stop_words.txt', 'r') as f:\n",
    "            ro_sw = f.readlines()\n",
    "            ro_sw = set(w.strip() for w in ro_sw)\n",
    "            PreprocessExpert.RO_STOP_WORDS = list(frozenset(ro_sw))\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stop_words_ro(doc):\n",
    "        if PreprocessExpert.RO_STOP_WORDS is None:\n",
    "            PreprocessExpert.load_stop_words_ro()\n",
    "        new_doc = ' '.join([w for w in doc.split() if w not in PreprocessExpert.RO_STOP_WORDS])\n",
    "        return new_doc\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_stop_words_en(doc):\n",
    "        # \n",
    "        new_doc = ' '.join([w for w in doc.split() if w not in stopwords.words()])\n",
    "        return new_doc\n",
    "\n",
    "    @staticmethod\n",
    "    def download_acronyms():\n",
    "        # https://www.netlingo.com/acronyms.php\n",
    "        # stack python-module-to-remove-internet-jargon-slang-acronym\n",
    "        # TODO? ro: http://abrevierile.ro/colectii\n",
    "        resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        slangdict = {}\n",
    "        key = \"\"\n",
    "        value = \"\"\n",
    "        for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "            for li in div.findAll('li'):\n",
    "                for a in li.findAll('a'):\n",
    "                    key = a.text\n",
    "                    value = li.text.split(key)[1]\n",
    "                    slangdict[key.lower()]=value\n",
    "\n",
    "        with open('data_files/acronym_en.json', 'w') as f:\n",
    "            json.dump(slangdict, f, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_acronyms():\n",
    "        # Loads acronyms data\n",
    "        if not os.path.exists('data_files/acronym_en.json'):\n",
    "            PreprocessExpert.download_acronyms()\n",
    "        with open('data_files/acronym_en.json', 'r') as f:\n",
    "            PreprocessExpert.EN_ACRONYMS = json.load(f)\n",
    "        # print(PreprocessExpert.EN_ACRONYMS)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_acronyms(doc):\n",
    "        # replace...\n",
    "        # if PreprocessExpert.EN_ACRONYMS is None:\n",
    "        #     PreprocessExpert.load_acronyms()\n",
    "        new_doc = ' '.join([w for w in doc.split() if w not in PreprocessExpert.EN_ACRONYMS.keys()])\n",
    "        return new_doc\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_email_addr(doc):\n",
    "        # Removes email strings from doc\n",
    "        new_doc = re.sub(r'[A-Za-z0-9]{1,}@[A-Za-z]{1,}\\.[A-Za-z0-9]{1,}', \"\", doc)\n",
    "        return new_doc\n",
    "\n",
    "    @staticmethod\n",
    "    def stemm_en(doc):\n",
    "        # Stemming en words of doc\n",
    "        new_doc = []\n",
    "        our_words = doc.split()\n",
    "        if PreprocessExpert.PORTER_STEM is None:\n",
    "            PreprocessExpert.PORTER_STEM = PorterStemmer()\n",
    "        for w in our_words:\n",
    "            # if word is of known en\n",
    "            if PreprocessExpert.is_english_word(w):\n",
    "                new_doc.append(PreprocessExpert.PORTER_STEM.stem(w))\n",
    "            else:\n",
    "                new_doc.append(w)\n",
    "        return ' '.join(new_doc)\n",
    "\n",
    "    @staticmethod\n",
    "    def lemma_en(doc):\n",
    "        # Lemmatization of en words in doc\n",
    "        new_doc = []\n",
    "        our_words = doc.split()\n",
    "        if PreprocessExpert.WORDNET_LEMMATIZER is None:\n",
    "            PreprocessExpert.WORDNET_LEMMATIZER = WordNetLemmatizer()\n",
    "        for w in our_words:\n",
    "            # if word is of known en\n",
    "            if PreprocessExpert.is_english_word(w):\n",
    "                new_doc.append(PreprocessExpert.WORDNET_LEMMATIZER.lemmatize(w))\n",
    "            else:\n",
    "                new_doc.append(w)\n",
    "        return ' '.join(new_doc)\n",
    "    \n",
    "    @staticmethod\n",
    "    def replace_ro_diacritics(doc):\n",
    "        diacritics_counterparts = {\"Ă\":\"A\",\n",
    "                        \"ă\":\"a\",\n",
    "                        \"Â\":\"A\",\n",
    "                        \"â\":\"a\",\n",
    "                        \"Î\": \"I\",\n",
    "                        \"î\": \"i\",\n",
    "                        \"Ș\":\"S\",\n",
    "                        \"ș\":\"s\",\n",
    "                        \"Ț\":\"T\",\n",
    "                        \"ț\":\"t\",\n",
    "                        \"Ş\":\"S\",\n",
    "                        \"ş\":\"s\"}\n",
    "        for index, letter in enumerate(doc):\n",
    "            if letter in diacritics_counterparts:\n",
    "                doc[index] = diacritics_counterparts[letter]\n",
    "        return doc\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_english_word(word):\n",
    "        return dict_en.check(word)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_ulrs(doc):\n",
    "        return re.sub(r'https?://\\S+', \"\", doc)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_hashtags(doc):\n",
    "        return re.sub(r'(@[A-Za-z0-9]+)', \"\", doc)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lemma_ro(text):\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            empty_list = []\n",
    "            for token in doc:\n",
    "                empty_list.append(token.lemma_)\n",
    "\n",
    "            final_string = ' '.join(map(str,empty_list))\n",
    "            return final_string\n",
    "        except:\n",
    "            return text\n",
    "        \n",
    "    @staticmethod\n",
    "    def nltk_stopwords_ro(phrases):\n",
    "        stopwords = nltk.corpus.stopwords.words('romanian')\n",
    "        removed_stopwords = [w for w in phrases if not w in stopwords]\n",
    "        return removed_stopwords\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_txt(doc):\n",
    "        # Normalize txt\n",
    "        new_doc = doc.lower()\n",
    "        # Removing email\n",
    "        new_doc = PreprocessExpert.remove_email_addr(new_doc)\n",
    "        # Removing urls\n",
    "        new_doc = PreprocessExpert.remove_ulrs(new_doc)\n",
    "        # Removing hashtags\n",
    "        new_doc = PreprocessExpert.remove_hashtags(new_doc)\n",
    "        # Replace diacritics\n",
    "        new_doc = PreprocessExpert.replace_ro_diacritics(new_doc)\n",
    "        # Removing symbols and such\n",
    "        new_doc = re.sub(r\"( -)|(- )|(\\d+)|([`~!@#$%^&*()_+[\\]{};:'\\\"\\\\|,<.>/?]+)\", \"\", new_doc)\n",
    "        new_doc = re.sub(r\"(a{2,})|(i{2,})|(o{2,})|(u{2,})|(e{3,})\", \"\", new_doc)\n",
    "        # Removing stop words - ro, en\n",
    "        new_doc = PreprocessExpert.remove_stop_words_ro(new_doc)\n",
    "        new_doc = PreprocessExpert.remove_stop_words_en(new_doc)\n",
    "        # Stemming en\n",
    "        new_doc = PreprocessExpert.stemm_en(new_doc)\n",
    "        # Lemmatization ro\n",
    "        new_doc = PreprocessExpert.get_lemma_ro(new_doc)\n",
    "        return new_doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20a0d209-6c98-431f-a526-c1e5787a14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fain messag merge\n"
     ]
    }
   ],
   "source": [
    "print(PreprocessExpert.clean_txt('Aa!! castron@gi.com e fain. Peste tot si toate... message, lol. Mereu mersese!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e38a4c-a9a4-4ad3-b7b2-ad671e2f186c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4a2c0-d7ca-416e-b8b7-3e35a6896cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feec0de-07a1-41b1-84f8-ce652ac59c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566e2d0-a184-49cd-957d-2e61960b0bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd0d75-91de-46fe-8c18-4bb2b9d8ba01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e4164-c876-43fd-893d-e5c585a6266f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238fd20-000c-48c1-83e9-8acac4f347b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
